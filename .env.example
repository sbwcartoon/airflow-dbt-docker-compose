# source db
SOURCE_POSTGRES_USER=postgres
SOURCE_POSTGRES_PASSWORD=postgres
SOURCE_POSTGRES_HOST=host.docker.internal
SOURCE_POSTGRES_PORT=5432
SOURCE_POSTGRES_DB=warehouse
## for airflow
AIRFLOW_CONN_SOURCE_DB="postgresql+psycopg2://${SOURCE_POSTGRES_USER}:${SOURCE_POSTGRES_PASSWORD}@${SOURCE_POSTGRES_HOST}:${SOURCE_POSTGRES_PORT}/${SOURCE_POSTGRES_DB}"
SOURCE_POSTGRES_SCHEMA=src

# airflow meta db
AIRFLOW_META_POSTGRES_USER=postgres
AIRFLOW_META_POSTGRES_PASSWORD=postgres
AIRFLOW_META_POSTGRES_HOST=host.docker.internal
AIRFLOW_META_POSTGRES_PORT=5432
AIRFLOW_META_POSTGRES_DB=warehouse
AIRFLOW_META_DB="postgresql+psycopg2://${AIRFLOW_META_POSTGRES_USER}:${AIRFLOW_META_POSTGRES_PASSWORD}@${AIRFLOW_META_POSTGRES_HOST}:${AIRFLOW_META_POSTGRES_PORT}/${AIRFLOW_META_POSTGRES_DB}"

# analytics db: local | prod
## dbt에서 DBT_TARGET이 local일 경우 LOCAL_* 정보를 사용하고, prod일 경우 PROD_* 정보를 사용함
DBT_TARGET=local
## local
LOCAL_ANALYTICS_GCP_PROJECT_ID=warehouse
LOCAL_ANALYTICS_GCP_LOCATION=US
LOCAL_GOOGLE_APPLICATION_CREDENTIALS=bigquery-service-account.json
LOCAL_ANALYTICS_GCP_DEFAULT_DATASET=dbt
## prod
PROD_ANALYTICS_GCP_PROJECT_ID=
PROD_ANALYTICS_GCP_LOCATION=
PROD_GOOGLE_APPLICATION_CREDENTIALS=
PROD_ANALYTICS_GCP_DEFAULT_DATASET=
## for airflow
ANALYTICS_GCP_PROJECT_ID="${LOCAL_ANALYTICS_GCP_PROJECT_ID}"
ANALYTICS_GCP_LOCATION="${LOCAL_ANALYTICS_GCP_LOCATION}"
GOOGLE_APPLICATION_CREDENTIALS="${LOCAL_GOOGLE_APPLICATION_CREDENTIALS}"
AIRFLOW_UID=

# airflow ssh to dbt
SSH_CONN_ID=ssh_dbt_conn
SSH_ID=airflow
SSH_PASSWORD=airflow
## generate ssh key by init-ssh-key.sh
SSH_HOST_KEY=

# ui port
AIRFLOW_APISERVER_PORT=8082
DBT_DOCS_PORT=8085